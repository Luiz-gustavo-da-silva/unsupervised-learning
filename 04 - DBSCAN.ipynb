{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - DBSCAN\n",
    "\n",
    "## Introdução\n",
    "\n",
    "O **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) é um algoritmo de clustering baseado em densidade desenvolvido por Martin Ester e colaboradores em 1996. Diferentemente do K-Means e clustering hierárquico, o DBSCAN não requer que especifiquemos o número de clusters antecipadamente e é capaz de identificar clusters de formas arbitrárias e detectar outliers (ruído).\n",
    "\n",
    "### Características Principais do DBSCAN:\n",
    "\n",
    "1. **Baseado em densidade**: Agrupa pontos que estão densamente empacotados\n",
    "2. **Detecção de ruído**: Identifica automaticamente outliers\n",
    "3. **Forma arbitrária**: Pode encontrar clusters de qualquer formato\n",
    "4. **Número automático de clusters**: Não precisa especificar k antecipadamente\n",
    "5. **Robusto a outliers**: Outliers não afetam a formação dos clusters\n",
    "\n",
    "## Fundamentos Matemáticos\n",
    "\n",
    "O DBSCAN utiliza dois parâmetros principais:\n",
    "- $\\varepsilon$ (eps): raio da vizinhança\n",
    "- $\\text{minPts}$: número mínimo de pontos para formar um cluster\n",
    "\n",
    "### Definições Fundamentais:\n",
    "\n",
    "**1. Vizinhança-$\\varepsilon$**: Para um ponto $p$, sua vizinhança-$\\varepsilon$ é definida como:\n",
    "$$N_\\varepsilon(p) = \\{q \\in D | \\text{dist}(p,q) \\leq \\varepsilon\\}$$\n",
    "\n",
    "**2. Ponto Central (Core Point)**: Um ponto $p$ é um ponto central se:\n",
    "$$|N_\\varepsilon(p)| \\geq \\text{minPts}$$\n",
    "\n",
    "**3. Diretamente Alcançável por Densidade**: Um ponto $q$ é diretamente alcançável por densidade a partir de $p$ se:\n",
    "- $q \\in N_\\varepsilon(p)$ e\n",
    "- $p$ é um ponto central\n",
    "\n",
    "**4. Alcançável por Densidade**: Um ponto $q$ é alcançável por densidade a partir de $p$ se existe uma cadeia de pontos $p_1, p_2, ..., p_n$ onde $p_1 = p$ e $p_n = q$, tal que $p_{i+1}$ é diretamente alcançável por densidade a partir de $p_i$.\n",
    "\n",
    "**5. Conectado por Densidade**: Dois pontos $p$ e $q$ são conectados por densidade se existe um ponto $o$ tal que tanto $p$ quanto $q$ são alcançáveis por densidade a partir de $o$.\n",
    "\n",
    "### Classificação dos Pontos:\n",
    "\n",
    "- **Core Point (Ponto Central)**: $|N_\\varepsilon(p)| \\geq \\text{minPts}$\n",
    "- **Border Point (Ponto de Fronteira)**: $|N_\\varepsilon(p)| < \\text{minPts}$, mas está na vizinhança de um core point\n",
    "- **Noise Point (Ponto de Ruído)**: Não é core nem border point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN as SklearnDBSCAN\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação do DBSCAN\n",
    "\n",
    "Vamos implementar o algoritmo DBSCAN passo a passo usando apenas NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCAN:\n",
    "    def __init__(self, eps=0.5, min_pts=5, metric='euclidean'):\n",
    "        \"\"\"Inicializa o DBSCAN com os parâmetros eps e min_pts\"\"\"\n",
    "        self.eps = eps\n",
    "        self.min_pts = min_pts\n",
    "        self.metric = metric\n",
    "        self.labels_ = None\n",
    "        self.core_samples_ = None\n",
    "        self.n_clusters_ = 0\n",
    "    \n",
    "    def _calculate_distance_matrix(self, X):\n",
    "        \"\"\"Calcula a matriz de distâncias entre todos os pontos\"\"\"\n",
    "        if self.metric == 'euclidean':\n",
    "            distances = np.linalg.norm(X[:, np.newaxis] - X, axis=2)\n",
    "        # elif self.metric == '...':\n",
    "            # distance = ...\n",
    "        else:\n",
    "            raise ValueError(\"Métrica não suportada\")\n",
    "        return distances\n",
    "    \n",
    "    def _get_neighbors(self, point_idx, distance_matrix):\n",
    "        \"\"\"Encontra todos os vizinhos dentro da distância eps\"\"\"\n",
    "        return np.where(distance_matrix[point_idx] <= self.eps)[0]\n",
    "    \n",
    "    def _expand_cluster(self, point_idx, neighbors, cluster_id, distance_matrix, visited, labels):\n",
    "        \"\"\"Expande o cluster a partir do ponto inicial\"\"\"\n",
    "        labels[point_idx] = cluster_id\n",
    "        queue = neighbors.tolist()\n",
    "\n",
    "        while queue:\n",
    "            neighbor_idx = queue.pop()\n",
    "\n",
    "            if not visited[neighbor_idx]:\n",
    "                visited[neighbor_idx] = True\n",
    "                neighbor_neighbors = self._get_neighbors(neighbor_idx, distance_matrix)\n",
    "\n",
    "                if len(neighbor_neighbors) >= self.min_pts:\n",
    "                    queue.extend(neighbor_neighbors)\n",
    "\n",
    "            if labels[neighbor_idx] == -1:\n",
    "                labels[neighbor_idx] = cluster_id\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Executa o algoritmo DBSCAN\"\"\"\n",
    "        n_points = len(X)\n",
    "        visited = np.zeros(n_points, dtype=bool)\n",
    "        cluster_id = 0\n",
    "        self.labels_ = np.full(n_points, -1)  # -1 = ruído\n",
    "        self.core_samples_ = []\n",
    "\n",
    "        distance_matrix = self._calculate_distance_matrix(X)\n",
    "\n",
    "        for point_idx in range(n_points):\n",
    "            if visited[point_idx]:\n",
    "                continue\n",
    "\n",
    "            visited[point_idx] = True\n",
    "            neighbors = self._get_neighbors(point_idx, distance_matrix)\n",
    "\n",
    "            if len(neighbors) >= self.min_pts:   # core point\n",
    "                self.core_samples_.append(point_idx)\n",
    "                self._expand_cluster(point_idx, neighbors, cluster_id, distance_matrix, visited, self.labels_)\n",
    "                cluster_id += 1\n",
    "\n",
    "        self.core_samples_ = np.array(self.core_samples_)\n",
    "        self.n_clusters_ = cluster_id\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Executa DBSCAN e retorna os labels\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstração com Dados Sintéticos\n",
    "\n",
    "Vamos criar dados sintéticos para demonstrar o funcionamento do DBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Cabeça: círculo com leve ruído\n",
    "n_head = 400\n",
    "theta = rng.uniform(0, 2*np.pi, n_head)\n",
    "R = 10 + rng.normal(0, 0.35, n_head)\n",
    "head = np.c_[R*np.cos(theta), R*np.sin(theta)]\n",
    "y_head = np.full(n_head, 0)\n",
    "\n",
    "# Olhos: dois blobs gaussianos\n",
    "n_eye = 100\n",
    "eye_left  = rng.normal(loc=[-3.2,  3.0], scale=[0.45, 0.45], size=(n_eye//2, 2))\n",
    "eye_right = rng.normal(loc=[ 3.2,  3.0], scale=[0.45, 0.45], size=(n_eye - n_eye//2, 2))\n",
    "eyes = np.vstack([eye_left, eye_right])\n",
    "y_eyes = np.full(eyes.shape[0], 1)\n",
    "\n",
    "# Boca: arco inferior com jitter (sorriso)\n",
    "n_mouth = 100\n",
    "phi = rng.uniform(np.deg2rad(200), np.deg2rad(340), n_mouth)  # arco de 200° a 340°\n",
    "Rm = 5 + rng.normal(0, 0.22, n_mouth)\n",
    "mouth = np.c_[Rm*np.cos(phi), -1 + Rm*np.sin(phi)]\n",
    "mouth += rng.normal(0, [0.12, 0.15], mouth.shape)  # engrossar um pouco\n",
    "y_mouth = np.full(n_mouth, 2)\n",
    "\n",
    "# Ruído: pontos aleatórios\n",
    "n_noise = 100\n",
    "noise = rng.uniform(low=[-13, -13], high=[13, 13], size=(n_noise, 2))\n",
    "y_noise = np.full(n_noise, -1)\n",
    "\n",
    "# Concatenar\n",
    "X_synthetic = np.vstack([head, eyes, mouth, noise])\n",
    "true_labels  = np.concatenate([y_head, y_eyes, y_mouth, y_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os dados sintéticos\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['red', 'blue', 'green', 'gray']\n",
    "for i in range(4):\n",
    "    if i == 3:  # ruído\n",
    "        mask = true_labels == -1\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], c=colors[i], alpha=0.6, s=30, marker='x', label='Ruído')\n",
    "    else:\n",
    "        mask = true_labels == i\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], c=colors[i], alpha=0.7, s=50, label=f'Cluster {i+1}')\n",
    "\n",
    "plt.title('Dados Sintéticos - Classes Verdadeiras')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_synthetic[:, 0], X_synthetic[:, 1], c='black', alpha=0.6, s=30)\n",
    "plt.title('Dados Sintéticos - Sem Labels')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.3, min_pts=7)\n",
    "labels = dbscan.fit_predict(X_synthetic)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "for k, label in enumerate(unique_labels):\n",
    "    if label == -1:\n",
    "        # Ruído\n",
    "        mask = labels == label\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], c='black', marker='x', s=30, alpha=0.6, label='Ruído')\n",
    "    else:\n",
    "        # Clusters\n",
    "        mask = labels == label\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], c=[colors[k]], s=50, alpha=0.7, label=f'Cluster {label}')\n",
    "\n",
    "# Destacar core points\n",
    "if len(dbscan.core_samples_) > 0:\n",
    "    plt.scatter(X_synthetic[dbscan.core_samples_, 0], \n",
    "                X_synthetic[dbscan.core_samples_, 1],\n",
    "                s=100, facecolors='none', edgecolors='black', \n",
    "                linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando DBSCAN aos Dados Sintéticos\n",
    "\n",
    "Agora vamos aplicar nosso algoritmo DBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar diferentes valores de eps e min_pts\n",
    "eps_values = [0.5, 1.3, 2.0]\n",
    "min_pts_values = [3, 5, 8]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "fig.suptitle('DBSCAN com Diferentes Parâmetros', fontsize=16)\n",
    "\n",
    "for i, eps in enumerate(eps_values):\n",
    "    for j, min_pts in enumerate(min_pts_values):\n",
    "        # Aplicar DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_pts=min_pts)\n",
    "        labels = dbscan.fit_predict(X_synthetic)\n",
    "        \n",
    "        # Visualizar resultados\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        unique_labels = np.unique(labels)\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for k, label in enumerate(unique_labels):\n",
    "            if label == -1:\n",
    "                # Ruído\n",
    "                mask = labels == label\n",
    "                ax.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                          c='black', marker='x', s=30, alpha=0.6, label='Ruído')\n",
    "            else:\n",
    "                # Clusters\n",
    "                mask = labels == label\n",
    "                ax.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                          c=[colors[k]], s=50, alpha=0.7, label=f'Cluster {label}')\n",
    "        \n",
    "        # Destacar core points\n",
    "        if len(dbscan.core_samples_) > 0:\n",
    "            ax.scatter(X_synthetic[dbscan.core_samples_, 0], \n",
    "                      X_synthetic[dbscan.core_samples_, 1],\n",
    "                      s=100, facecolors='none', edgecolors='black', \n",
    "                      linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'eps={eps}, min_pts={min_pts}\\n{dbscan.n_clusters_} clusters')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise com Parâmetros Ótimos\n",
    "\n",
    "Vamos escolher os parâmetros que melhor capturam a estrutura dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros que parecem funcionar melhor\n",
    "best_eps = 1.3\n",
    "best_min_pts = 5\n",
    "\n",
    "# Aplicar DBSCAN com os melhores parâmetros\n",
    "dbscan_best = DBSCAN(eps=best_eps, min_pts=best_min_pts)\n",
    "labels_best = dbscan_best.fit_predict(X_synthetic)\n",
    "\n",
    "# Análise detalhada\n",
    "unique_labels = np.unique(labels_best)\n",
    "n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "n_noise = np.sum(labels_best == -1)\n",
    "n_core_samples = len(dbscan_best.core_samples_)\n",
    "\n",
    "print(f\"Resultados do DBSCAN (eps={best_eps}, min_pts={best_min_pts}):\")\n",
    "print(f\"- Número de clusters encontrados: {n_clusters}\")\n",
    "print(f\"- Número de pontos de ruído: {n_noise}\")\n",
    "print(f\"- Número de core samples: {n_core_samples}\")\n",
    "print(f\"- Labels únicos: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificar pontos por tipo\n",
    "core_mask = np.zeros(len(X_synthetic), dtype=bool)\n",
    "if len(dbscan_best.core_samples_) > 0:\n",
    "    core_mask[dbscan_best.core_samples_] = True\n",
    "\n",
    "border_mask = (labels_best != -1) & (~core_mask)\n",
    "noise_mask = labels_best == -1\n",
    "\n",
    "print(f\"Classificação dos pontos:\")\n",
    "print(f\"- Core points: {np.sum(core_mask)}\")\n",
    "print(f\"- Border points: {np.sum(border_mask)}\")\n",
    "print(f\"- Noise points: {np.sum(noise_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização detalhada dos tipos de pontos\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Clusters encontrados\n",
    "plt.subplot(1, 3, 1)\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, max(len(unique_labels), 3)))\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    if label == -1:\n",
    "        mask = labels_best == label\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                   c='black', marker='x', s=50, alpha=0.8, label='Ruído')\n",
    "    else:\n",
    "        mask = labels_best == label\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                   c=[colors[i]], s=60, alpha=0.8, label=f'Cluster {label}')\n",
    "\n",
    "plt.title(f'DBSCAN - Clusters Encontrados\\n{n_clusters} clusters, {n_noise} ruído')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Tipos de pontos\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_synthetic[core_mask, 0], X_synthetic[core_mask, 1], \n",
    "           c='red', s=80, alpha=0.8, label='Core Points', marker='o')\n",
    "plt.scatter(X_synthetic[border_mask, 0], X_synthetic[border_mask, 1], \n",
    "           c='blue', s=60, alpha=0.8, label='Border Points', marker='s')\n",
    "plt.scatter(X_synthetic[noise_mask, 0], X_synthetic[noise_mask, 1], \n",
    "           c='black', s=40, alpha=0.8, label='Noise Points', marker='x')\n",
    "\n",
    "plt.title('Classificação dos Pontos')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Comparação com ground truth\n",
    "plt.subplot(1, 3, 3)\n",
    "for i in range(4):\n",
    "    if i == 3:  # ruído\n",
    "        mask = true_labels == -1\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                   c='gray', alpha=0.6, s=30, marker='x', label='Ruído Real')\n",
    "    else:\n",
    "        mask = true_labels == i\n",
    "        plt.scatter(X_synthetic[mask, 0], X_synthetic[mask, 1], \n",
    "                   c=colors[i], alpha=0.7, s=50, label=f'Cluster Real {i+1}')\n",
    "\n",
    "plt.title('Ground Truth')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimativa do Parâmetro $\\varepsilon$ usando K-Distance\n",
    "\n",
    "Uma das maiores dificuldades do DBSCAN é escolher o valor apropriado para o parâmetro $\\varepsilon$ (eps). O método **K-Distance** é uma heurística eficaz para estimar este parâmetro.\n",
    "\n",
    "O método K-Distance consiste em:\n",
    "\n",
    "1. **Calcular a k-ésima distância mais próxima** para cada ponto no dataset\n",
    "2. **Ordenar essas distâncias** em ordem decrescente\n",
    "3. **Identificar o \"cotovelo\"** no gráfico resultante\n",
    "\n",
    "A intuição é que pontos dentro de clusters densos terão k-ésimas distâncias pequenas, enquanto pontos de ruído ou em bordas de clusters terão distâncias maiores.\n",
    "\n",
    "### Algoritmo K-Distance:\n",
    "\n",
    "Para um dataset $D$ e parâmetro $k = \\text{minPts} - 1$:\n",
    "\n",
    "1. Para cada ponto $p_i \\in D$:\n",
    "   - Calcule $d_k(p_i)$ = distância ao k-ésimo vizinho mais próximo\n",
    "2. Ordene os valores $d_k(p_i)$\n",
    "3. Plote o gráfico K-Distance\n",
    "4. Escolha $\\varepsilon$ no ponto onde a curva tem maior curvatura (cotovelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def plot_k_distance(X, min_pts, title=\"K-Distance Plot\"):\n",
    "    \"\"\"Plota o gráfico K-Distance usando sklearn.NearestNeighbors.\"\"\"\n",
    "    k = int(min_pts - 1)\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=k+1, metric=\"euclidean\")\n",
    "    nn.fit(X)\n",
    "    distances, _ = nn.kneighbors(X)\n",
    "\n",
    "    kth_distances = distances[:, k]\n",
    "    k_distances_sorted = np.sort(kth_distances)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(k_distances_sorted)), k_distances_sorted, linewidth=2, label=f'{k}-distance')\n",
    "    plt.xlabel(\"Pontos ordenados por distância\")\n",
    "    plt.ylabel(f\"{k}-distance\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_distance(X_synthetic, min_pts=5, title=\"K-Distance Plot para Dados Sintéticos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vantagens e Desvantagens do DBSCAN\n",
    "\n",
    "### Vantagens:\n",
    "\n",
    "1. **Não requer especificar o número de clusters antecipadamente**\n",
    "2. **Pode encontrar clusters de forma arbitrária** (não apenas esféricos)\n",
    "3. **Identifica automaticamente outliers/ruído**\n",
    "4. **Robusto a outliers** (não afetam a formação dos clusters)\n",
    "5. **Determinístico** (sempre produz os mesmos resultados)\n",
    "\n",
    "### Desvantagens:\n",
    "\n",
    "1. **Sensível aos parâmetros** eps e min_pts\n",
    "2. **Dificuldade com clusters de densidades diferentes**\n",
    "3. **Problemas em alta dimensionalidade** (\"curse of dimensionality\")\n",
    "4. **Complexidade computacional** O(n²) no pior caso\n",
    "5. **Requer escolha cuidadosa da métrica de distância**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1: Ajuste de Parâmetros no DBSCAN em 3D\n",
    "\n",
    "Com os dados das **três esferas concêntricas**, realize:\n",
    "\n",
    "1. Plotar o K-Distance para diferentes valores de `min_pts` e sugerir um intervalo adequado para `eps`.\n",
    "2. Selecionar os melhores parâmetros de `min_pts` e `eps`.\n",
    "3. Visualizar em 3D os clusters encontrados (cores diferentes) e comentar a escolha de `eps` e `min_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def generate_concentric_spheres(radii=[3, 15], n_samples_per_sphere=1000, noise=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Gera pontos em 3 esferas concêntricas no espaço 3D.\n",
    "    - radii: lista com os raios das esferas\n",
    "    - n_samples_per_sphere: pontos em cada esfera\n",
    "    - noise: variação radial para \"espessura\" da casca\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i, r in enumerate(radii):\n",
    "        # amostrar ângulos uniformemente\n",
    "        phi = rng.uniform(0, 2*np.pi, n_samples_per_sphere)       # ângulo azimutal\n",
    "        costheta = rng.uniform(-1, 1, n_samples_per_sphere)       # cos(theta)\n",
    "        theta = np.arccos(costheta)                               # ângulo polar\n",
    "        \n",
    "        # raio com ruído\n",
    "        rr = r + noise * rng.standard_normal(n_samples_per_sphere)\n",
    "        \n",
    "        # coordenadas cartesianas\n",
    "        x = rr * np.sin(theta) * np.cos(phi)\n",
    "        y_ = rr * np.sin(theta) * np.sin(phi)\n",
    "        z = rr * np.cos(theta)\n",
    "        \n",
    "        X.append(np.vstack((x, y_, z)).T)\n",
    "        y.append(np.full(n_samples_per_sphere, i))\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.concatenate(y)\n",
    "    return X, y\n",
    "\n",
    "X_spheres, y_spheres = generate_concentric_spheres(radii=[3, 8, 12], n_samples_per_sphere=200, noise=0.4)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_spheres = scaler.fit_transform(X_spheres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    x=X_spheres[:, 0],\n",
    "    y=X_spheres[:, 1],\n",
    "    z=X_spheres[:, 2],\n",
    "    color_continuous_scale=px.colors.qualitative.Vivid,\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "import plotly.express as px\n",
    "\n",
    "# ---------- Gerar as 3 esferas concêntricas ----------\n",
    "def generate_concentric_spheres(radii=[3, 8, 12], n_samples_per_sphere=200, noise=0.4, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X, y = [], []\n",
    "\n",
    "    for i, r in enumerate(radii):\n",
    "        phi = rng.uniform(0, 2*np.pi, n_samples_per_sphere)\n",
    "        costheta = rng.uniform(-1, 1, n_samples_per_sphere)\n",
    "        theta = np.arccos(costheta)\n",
    "        rr = r + noise * rng.standard_normal(n_samples_per_sphere)\n",
    "        x = rr * np.sin(theta) * np.cos(phi)\n",
    "        y_ = rr * np.sin(theta) * np.sin(phi)\n",
    "        z = rr * np.cos(theta)\n",
    "        X.append(np.vstack((x, y_, z)).T)\n",
    "    X = np.vstack(X)\n",
    "    return X\n",
    "\n",
    "X_spheres = generate_concentric_spheres()\n",
    "X_spheres = StandardScaler().fit_transform(X_spheres)\n",
    "\n",
    "def plot_k_distance(X, min_pts, title=\"K-Distance Plot\"):\n",
    "    k = int(min_pts - 1)\n",
    "    nn = NearestNeighbors(n_neighbors=k+1)\n",
    "    nn.fit(X)\n",
    "    distances, _ = nn.kneighbors(X)\n",
    "    kth_distances = np.sort(distances[:, k])\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(kth_distances, linewidth=2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Pontos ordenados\")\n",
    "    plt.ylabel(f\"{k}-distance\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "for min_pts in [5, 8, 9]:\n",
    "    plot_k_distance(X_spheres, min_pts, title=f\"K-Distance (min_pts={min_pts})\")\n",
    "\n",
    "best_eps = 0.7\n",
    "best_min_pts = 9\n",
    "\n",
    "db = DBSCAN(eps=best_eps, min_samples=best_min_pts)\n",
    "labels = db.fit_predict(X_spheres)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    x=X_spheres[:,0], y=X_spheres[:,1], z=X_spheres[:,2],\n",
    "    color=labels.astype(str),\n",
    "    title=f\"DBSCAN 3D - eps={best_eps}, min_samples={best_min_pts}\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n",
    "\n",
    "# O eps = 0.7 e o min_pts = 9.\n",
    "# Com esses valores foram encontrados 4 cluters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2: DBSCAN com distância radial\n",
    "\n",
    "Usando os dados das **3 esferas concêntricas** do exercício anterior:\n",
    "\n",
    "1. Implemente a **distância radial** e use-a no DBSCAN. A **distância radial** entre dois pontos \\(x_i\\) e \\(x_j\\) é a diferença absoluta entre suas distâncias à origem: $d_{\\text{radial}}(x_i, x_j) = \\big|\\;\\|x_i\\|_2 - \\|x_j\\|_2\\;\\big|$\n",
    "2. Plote o **K-Distance radial** para sugerir `eps`.  \n",
    "3. Teste combinações de `eps` e `min_samples`.  \n",
    "4. Visualize em 3D os clusters obtidos e compare com o resultado usando distância euclidiana.  \n",
    "5. Comente brevemente qual configuração foi melhor e por quê a métrica radial ajuda nesse dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercício 2: DBSCAN com distância radial ---\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Definindo distância radial\n",
    "def radial_distance(X):\n",
    "    norms = np.linalg.norm(X, axis=1).reshape(-1, 1)\n",
    "    return pairwise_distances(norms, norms, metric=\"euclidean\")\n",
    "\n",
    "class DBSCANRadial(DBSCAN):\n",
    "    def _calculate_distance_matrix(self, X):\n",
    "        return radial_distance(X)\n",
    "\n",
    "# 1) Plotando K-Distance radial\n",
    "plot_k_distance(X_spheres, min_pts=5, title=\"K-Distance (Radial)\")\n",
    "\n",
    "# 2) Testando DBSCAN radial\n",
    "db_radial = DBSCANRadial(eps=0.5, min_pts=5)\n",
    "labels_radial = db_radial.fit_predict(X_spheres)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    x=X_spheres[:, 0], y=X_spheres[:, 1], z=X_spheres[:, 2],\n",
    "    color=labels_radial.astype(str), opacity=0.7\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n",
    "\n",
    "print(f\"Clusters encontrados com radial: {db_radial.n_clusters_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import DBSCAN as SkDBSCAN\n",
    "\n",
    "\n",
    "def radial_distance_matrix(X):\n",
    "    # distância de cada ponto até a origem\n",
    "    r = np.linalg.norm(X, axis=1)\n",
    "    # matriz de distâncias radiais\n",
    "    return squareform(pdist(r[:, None], metric=\"euclidean\"))\n",
    "\n",
    "def plot_k_distance_radial(X, min_pts):\n",
    "    D = radial_distance_matrix(X)\n",
    "    k = int(min_pts - 1)\n",
    "    kth_distances = np.sort(np.partition(D, k, axis=1)[:, k])\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(kth_distances, linewidth=2)\n",
    "    plt.title(f\"K-Distance (Radial) - min_pts={min_pts}\")\n",
    "    plt.xlabel(\"Pontos ordenados\")\n",
    "    plt.ylabel(f\"{k}-distance (radial)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_k_distance_radial(X_spheres, min_pts=5)\n",
    "\n",
    "# Testando diferentes combinações\n",
    "D = radial_distance_matrix(X_spheres)\n",
    "\n",
    "params = [(0.15, 5), (0.2, 5), (0.25, 6), (0.3, 8), (0.4, 8)]\n",
    "resultados = []\n",
    "\n",
    "for eps, min_pts in params:\n",
    "    db_radial = SkDBSCAN(eps=eps, min_samples=min_pts, metric='precomputed')\n",
    "    labels_radial = db_radial.fit_predict(D)\n",
    "    \n",
    "    n_clusters = len(set(labels_radial)) - (1 if -1 in labels_radial else 0)\n",
    "    n_noise = np.sum(labels_radial == -1)\n",
    "    \n",
    "    resultados.append((eps, min_pts, n_clusters, n_noise))\n",
    "    print(f\"eps={eps}, min_samples={min_pts} → clusters: {n_clusters}, ruído: {n_noise}\")\n",
    "\n",
    "# A melhor\n",
    "best_eps = 0.25\n",
    "best_min_samples = 6\n",
    "\n",
    "db_radial = SkDBSCAN(eps=best_eps, min_samples=best_min_samples, metric='precomputed')\n",
    "labels_radial = db_radial.fit_predict(D)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    x=X_spheres[:,0], y=X_spheres[:,1], z=X_spheres[:,2],\n",
    "    color=labels_radial.astype(str),\n",
    "    title=f\"DBSCAN 3D - Distância Radial (eps={best_eps}, min_samples={best_min_samples})\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Safe\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n",
    "\n",
    "print(f\"Número de clusters encontrados (Radial): {len(set(labels_radial)) - (1 if -1 in labels_radial else 0)}\")\n",
    "print(f\"Pontos de ruído (Radial): {np.sum(labels_radial==-1)}\")\n",
    "\n",
    "db_euclid = SkDBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "labels_euclid = db_euclid.fit_predict(X_spheres)\n",
    "\n",
    "fig2 = px.scatter_3d(\n",
    "    x=X_spheres[:,0], y=X_spheres[:,1], z=X_spheres[:,2],\n",
    "    color=labels_euclid.astype(str),\n",
    "    title=f\"DBSCAN 3D - Distância Euclidiana (eps={best_eps}, min_samples={best_min_samples})\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Safe\n",
    ")\n",
    "fig2.update_traces(marker=dict(size=3))\n",
    "fig2.show()\n",
    "\n",
    "print(f\"Número de clusters encontrados (Euclidiana): {len(set(labels_euclid)) - (1 if -1 in labels_euclid else 0)}\")\n",
    "print(f\"Pontos de ruído (Euclidiana): {np.sum(labels_euclid==-1)}\")\n",
    "\n",
    "# A melhor combinação foi eps = 0.25 e min_samples = 6 \n",
    "# A distância radial se mostra mais eficiente neste caso porque leva em conta apenas\n",
    "# o quanto cada ponto está distante do centro, sem considerar o ângulo ou direção.\n",
    "# Logo, pontos que estão no mesmo raio são vistos como próximos,\n",
    "# mesmo que estejam em lados opostos da esfera.\n",
    "# Já a distância euclidiana considera também a posição angular,\n",
    "# o que faz com que pontos da mesma esfera pareçam mais afastados do que realmente estão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3: Detecção de Anomalias com DBSCAN e DTW\n",
    "\n",
    "O **DTW (Dynamic Time Warping)** mede a similaridade entre séries temporais mesmo quando estão defasadas ou com velocidades diferentes, alinhando-as de forma elástica. Isso permite detectar padrões semelhantes sem que a defasagem atrapalhe.\n",
    "\n",
    "Pode ser calculado por:\n",
    "```python\n",
    "from dtaidistance import dtw\n",
    "\n",
    "n = len(X)              # número de séries\n",
    "D = np.zeros((n, n))    # matriz de distâncias\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        dist = dtw.distance_fast(X[i], X[j])  # distância DTW\n",
    "        D[i, j] = D[j, i] = dist              # matriz simétrica\n",
    "````\n",
    "\n",
    "**Tarefas:**\n",
    "1. Use o dataset de senóides com variação e **anomalias simuladas**.  \n",
    "2. Adicione a métrica DTW no DBSCAN.\n",
    "3. Experimente diferentes valores de `eps` e `min_samples` até que o modelo consiga separar bem séries normais das anômalas.  \n",
    "4. Plote todas as séries, usando uma cor para as normais e outra para as anomalias detectadas (`label = -1`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series_dataset(n_series=50, length=100, noise=0.1, n_outliers=2, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X, y = [], []\n",
    "    t = np.linspace(0, 4*np.pi, length)\n",
    "\n",
    "    # séries normais: senóide com amplitude e frequência ligeiramente diferentes\n",
    "    for _ in range(n_series):\n",
    "        amp = rng.uniform(0.8, 1.2)         # amplitude\n",
    "        freq = rng.uniform(0.9, 1.1)        # frequência\n",
    "        phase = rng.uniform(0, 0.5*np.pi)   # pequena defasagem\n",
    "        series = amp * np.sin(freq * t + phase) + noise * rng.normal(size=length)\n",
    "        X.append(series)\n",
    "        y.append(0)  # normal\n",
    "\n",
    "    # outliers: picos ou deslocamentos fortes\n",
    "    for _ in range(n_outliers):\n",
    "        amp = rng.uniform(1.5, 2.0)         # amplitude anômala\n",
    "        freq = rng.uniform(1.2, 1.5)        # frequência anômala\n",
    "        series = amp * np.sin(freq * t) + noise * rng.normal(size=length)\n",
    "        if rng.random() < 0.5:\n",
    "            series[length//2] += 3  # pico\n",
    "        else:\n",
    "            series += rng.normal(2.0, 0.5)  # deslocamento\n",
    "        X.append(series)\n",
    "        y.append(-1)  # anomalia\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_series, y_series = generate_time_series_dataset()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "for i in range(5):\n",
    "    plt.plot(X_series[i], alpha=0.7, label=\"normal\" if i==0 else \"\")\n",
    "for i in range(-3,0):\n",
    "    plt.plot(X_series[i], alpha=0.7, color=\"red\", label=\"anomalia\" if i==-1 else \"\")\n",
    "plt.title(\"Séries temporais com anomalias\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Exercício 3: DBSCAN com DTW ---\n",
    "\n",
    "from dtaidistance import dtw\n",
    "\n",
    "# 1) Construir matriz de distâncias DTW\n",
    "n = len(X_series)\n",
    "D = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        dist = dtw.distance_fast(X_series[i], X_series[j])\n",
    "        D[i, j] = D[j, i] = dist\n",
    "\n",
    "# 2) Rodar DBSCAN com matriz DTW\n",
    "eps, min_samples = 10, 3\n",
    "db_dtw = SklearnDBSCAN(eps=eps, min_samples=min_samples, metric=\"precomputed\")\n",
    "labels_dtw = db_dtw.fit_predict(D)\n",
    "\n",
    "# 3) Visualizar resultados\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i, series in enumerate(X_series):\n",
    "    if labels_dtw[i] == -1:\n",
    "        plt.plot(series, color=\"red\", alpha=0.7, label=\"Anomalia\" if i == list(labels_dtw).index(-1) else \"\")\n",
    "    else:\n",
    "        plt.plot(series, color=\"blue\", alpha=0.5, label=\"Normal\" if i == 0 else \"\")\n",
    "plt.title(\"DBSCAN + DTW - Detecção de Anomalias\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Clusters encontrados: {len(set(labels_dtw)) - (1 if -1 in labels_dtw else 0)}\")\n",
    "print(f\"Anomalias detectadas: {np.sum(labels_dtw == -1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dtaidistance\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dtaidistance import dtw\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def generate_time_series_dataset(n_series=50, length=100, noise=0.1, n_outliers=2, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X, y = [], []\n",
    "    t = np.linspace(0, 4*np.pi, length)\n",
    "\n",
    "    for _ in range(n_series):\n",
    "        amp = rng.uniform(0.8, 1.2)\n",
    "        freq = rng.uniform(0.9, 1.1)\n",
    "        phase = rng.uniform(0, 0.5*np.pi)\n",
    "        series = amp * np.sin(freq * t + phase) + noise * rng.normal(size=length)\n",
    "        X.append(series)\n",
    "        y.append(0)\n",
    "\n",
    "    for _ in range(n_outliers):\n",
    "        amp = rng.uniform(1.5, 2.0)\n",
    "        freq = rng.uniform(1.2, 1.5)\n",
    "        series = amp * np.sin(freq * t) + noise * rng.normal(size=length)\n",
    "        if rng.random() < 0.5:\n",
    "            series[length//2] += 3\n",
    "        else:\n",
    "            series += rng.normal(2.0, 0.5)\n",
    "        X.append(series)\n",
    "        y.append(-1)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_series, y_series = generate_time_series_dataset()\n",
    "\n",
    "n = len(X_series)\n",
    "D = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        dist = dtw.distance_fast(X_series[i], X_series[j])\n",
    "        D[i, j] = D[j, i] = dist\n",
    "\n",
    "db_dtw = DBSCAN(eps=15, min_samples=3, metric=\"precomputed\")\n",
    "labels_dtw = db_dtw.fit_predict(D)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i, series in enumerate(X_series):\n",
    "    if labels_dtw[i] == -1:\n",
    "        plt.plot(series, color='red', alpha=0.7)\n",
    "    else:\n",
    "        plt.plot(series, color='blue', alpha=0.4)\n",
    "plt.title(\"Detecção de Anomalias com DBSCAN + DTW\")\n",
    "plt.xlabel(\"Tempo\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Clusters encontrados: {len(set(labels_dtw)) - (1 if -1 in labels_dtw else 0)}\")\n",
    "print(f\"Anomalias detectadas: {np.sum(labels_dtw==-1)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
